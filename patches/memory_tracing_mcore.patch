From 423ecc658ae32b90982a1ea218df5b10b204cf39 Mon Sep 17 00:00:00 2001
From: Dennis Liu <denliu@nvidia.com>
Date: Thu, 29 May 2025 01:13:10 -0700
Subject: [PATCH] Implement memory tracing feature in Megatron-LM

- Added `memory_tracing` configuration option to enable memory tracing during training.
- Updated various components to support memory tracing, including model setup, optimizer steps, and training iterations.
- Introduced a function to update arguments for memory tracing mode.
- Integrated `MemoryTracer` for tracking memory usage and added conditions to handle gradient clipping and tensor operations based on memory tracing status.
- Enhanced the `TransformerConfig` and `OptimizerConfig` classes to accommodate memory tracing settings.

Signed-off-by: Dennis Liu <denliu@nvidia.com>
---
 megatron/core/optimizer/optimizer.py            | 32 +++++-----
 megatron/core/optimizer/optimizer_config.py     |  3 +
 megatron/core/transformer/moe/experts.py        | 11 +++-
 megatron/core/transformer/transformer_block.py  |  2 +-
 megatron/core/transformer/transformer_config.py | 27 ++++++++
 megatron/training/arguments.py                  | 63 +++++++++++++++++++
 megatron/training/initialize.py                 |  6 ++
 megatron/training/training.py                   | 84 ++++++++++++++++---------
 pretrain_gpt.py                                 | 40 ++++++++++++
 9 files changed, 222 insertions(+), 46 deletions(-)

diff --git a/megatron/core/optimizer/optimizer.py b/megatron/core/optimizer/optimizer.py
index 5d2ec82..72c1c8a 100644
--- a/megatron/core/optimizer/optimizer.py
+++ b/megatron/core/optimizer/optimizer.py
@@ -1126,22 +1126,26 @@ class ChainedOptimizer(MegatronOptimizer):
         if found_inf_flag:
             return False, None, None
 
-        grad_norm = self.get_grad_norm()
+        if not self.config.memory_tracing:
+            grad_norm = self.get_grad_norm()
 
-        # Clip gradients.
-        for optimizer in self.chained_optimizers:
-            if hasattr(optimizer, 'is_stub_optimizer') and optimizer.is_stub_optimizer:
-                continue
-            if optimizer.config.clip_grad > 0.0:
-                clip_grad_by_total_norm_fp32(
-                    optimizer.get_parameters(),
-                    max_norm=optimizer.config.clip_grad,
-                    total_norm=grad_norm,
-                    use_decoupled_grad=optimizer.config.use_precision_aware_optimizer,
-                )
+            # Clip gradients.
+            for optimizer in self.chained_optimizers:
+                if hasattr(optimizer, 'is_stub_optimizer') and optimizer.is_stub_optimizer:
+                    continue
+                if optimizer.config.clip_grad > 0.0:
+                    clip_grad_by_total_norm_fp32(
+                        optimizer.get_parameters(),
+                        max_norm=optimizer.config.clip_grad,
+                        total_norm=grad_norm,
+                        use_decoupled_grad=optimizer.config.use_precision_aware_optimizer,
+                    )
 
-        # Count the zeros in the grads.
-        num_zeros_in_grad = self.count_zeros()
+            # Count the zeros in the grads.
+            num_zeros_in_grad = self.count_zeros()
+        else:
+            grad_norm = None
+            num_zeros_in_grad = None
 
         update_successful = self.step_with_ready_grads()
 
diff --git a/megatron/core/optimizer/optimizer_config.py b/megatron/core/optimizer/optimizer_config.py
index 5bcaaf5..bfd11b8 100644
--- a/megatron/core/optimizer/optimizer_config.py
+++ b/megatron/core/optimizer/optimizer_config.py
@@ -152,6 +152,9 @@ class OptimizerConfig:
     ################
     # Miscellaneous
     ################
+    memory_tracing: bool = False
+    """If true, enable memory tracing."""
+
     clip_grad: float = 1.0
     """Gradient clipping based on global L2 norm."""
 
diff --git a/megatron/core/transformer/moe/experts.py b/megatron/core/transformer/moe/experts.py
index f68df4b..07c0605 100644
--- a/megatron/core/transformer/moe/experts.py
+++ b/megatron/core/transformer/moe/experts.py
@@ -746,6 +746,9 @@ class TEGroupedMLP(MegatronModule):
             output (torch.Tensor): The output of the local experts.
         """
         tokens_per_expert = tokens_per_expert.tolist()
+        if self.config.moe_pad_expert_input_to_capacity:
+            num_tokens = permuted_local_hidden_states.shape[0]
+            tokens_per_expert = [num_tokens // self.num_local_experts] * self.num_local_experts
         if self.config.fp8:
             actual_tokens_per_expert = tokens_per_expert
             permuted_local_hidden_states, tokens_per_expert = self.fp8_padding(
@@ -949,8 +952,12 @@ class SequentialMLP(MegatronModule):
             return output, output_bias
         else:
             tokens_per_expert = tokens_per_expert.tolist()
-            tokens_list = torch.split(permuted_local_hidden_states, tokens_per_expert)
-            probs_list = torch.split(permuted_probs, tokens_per_expert)
+            if self.config.moe_pad_expert_input_to_capacity:
+                tokens_list = torch.chunk(permuted_local_hidden_states, len(tokens_per_expert), 0)
+                probs_list = torch.chunk(permuted_probs, len(tokens_per_expert), 0)
+            else:
+                tokens_list = torch.split(permuted_local_hidden_states, tokens_per_expert)
+                probs_list = torch.split(permuted_probs, tokens_per_expert)
 
             output_local_list = []
             output_bias_list = []
diff --git a/megatron/core/transformer/transformer_block.py b/megatron/core/transformer/transformer_block.py
index 1074b91..b0f6d6a 100755
--- a/megatron/core/transformer/transformer_block.py
+++ b/megatron/core/transformer/transformer_block.py
@@ -523,7 +523,7 @@ class TransformerBlock(MegatronModule):
         #   is called here to be future-proof and corner-case-proof.
         hidden_states = make_viewless_tensor(inp=hidden_states, requires_grad=True, keep_graph=True)
 
-        if self.config.sequence_parallel:
+        if self.config.sequence_parallel and not self.config.memory_tracing:
             rng_context = tensor_parallel.get_cuda_rng_tracker().fork()
         else:
             rng_context = nullcontext()
diff --git a/megatron/core/transformer/transformer_config.py b/megatron/core/transformer/transformer_config.py
index 6ba0199..247828f 100644
--- a/megatron/core/transformer/transformer_config.py
+++ b/megatron/core/transformer/transformer_config.py
@@ -29,6 +29,27 @@ class TransformerConfig(ModelParallelConfig):
     including those in ModelParallelConfig.
     """
 
+    def _update_for_memory_tracing(self):
+        """Update the config for memory tracing."""
+        self.no_persist_layer_norm = True
+        self.tp_comm_overlap = False
+        self.apply_rope_fusion = False
+        self.moe_permute_fusion = False
+
+        # FP8 related
+        assert self.fp8 is None, "fp8 is not supported in memory tracing"
+
+        # MoE related
+        if self.moe_expert_capacity_factor is None:
+            self.moe_expert_capacity_factor = 1.0
+            print("Warning: moe_expert_capacity_factor is not set, using 1.0 for memory tracing.")
+        self.moe_pad_expert_input_to_capacity = True
+
+        if self.moe_token_dispatcher_type != "alltoall":
+            self.moe_token_dispatcher_type = "alltoall"
+            print("Warning: moe_token_dispatcher_type is set to alltoall for memory tracing.")
+        self.moe_enable_deepep = False
+
     ####################
     # model architecture
     ####################
@@ -532,6 +553,9 @@ class TransformerConfig(ModelParallelConfig):
     config_logger_dir: str = ""
     """When non-empty, dumps entry-point configs to config_logger_dir"""
 
+    memory_tracing: bool = False
+    """If set, memory tracing mode is enabled and no actual computation is performed."""
+
     flash_decode: bool = False
     """ Use the optimized flash decoding kernel during inference. """
 
@@ -1101,6 +1125,9 @@ class TransformerConfig(ModelParallelConfig):
                     self.cp_comm_type, str
                 ), "Unsupported communication type for context parallelism!"
 
+        if self.memory_tracing:
+            self._update_for_memory_tracing()
+
         assert (
             self.pipeline_model_parallel_size > 0
         ), f"Pipeline model parallel size must be larger than 0 \
diff --git a/megatron/training/arguments.py b/megatron/training/arguments.py
index e62901d..bb01e93 100644
--- a/megatron/training/arguments.py
+++ b/megatron/training/arguments.py
@@ -318,6 +318,62 @@ def moe_freq_type(x):
         return int(x)
 
 
+def update_arguments_for_memory_tracing(args):
+    # Setting only training for 1 step
+    args.eval_iters = 0
+    args.train_iters = 2
+    args.do_train = True
+
+    # Reducing global batch size to a enough value
+    dp_size = args.world_size // (args.context_parallel_size * args.tensor_model_parallel_size * args.pipeline_model_parallel_size)
+    num_micro_batches = args.global_batch_size // (dp_size * args.micro_batch_size) // args.micro_batch_size
+    min_num_micro_batches = args.pipeline_model_parallel_size * 2
+    if min_num_micro_batches < num_micro_batches:
+        args.global_batch_size = args.global_batch_size * min_num_micro_batches // num_micro_batches
+
+    # Checkpointing related
+    args.save = None
+    args.load = None
+
+    # Data related
+    args.data_path = None
+    args.mock_data = True
+
+    # Distributed related
+    args.enable_gloo_process_groups = False
+
+    # Logging related
+    args.gradient_accumulation_fusion = False
+    args.check_for_nan_in_loss_and_grad = False
+    args.log_params_norm = False
+    args.log_timers_to_tensorboard = False
+    args.tensorboard_dir = None
+    args.wandb_project = None
+
+    # Kernel related
+    args.no_persist_layer_norm = True
+    args.tp_comm_overlap = False
+    args.apply_rope_fusion = False
+    args.moe_permute_fusion = False
+
+    # FP8 related
+    assert args.fp8 is None, "fp8 is not supported in memory tracing"
+
+    # MoE related
+    if args.moe_expert_capacity_factor is None:
+        args.moe_expert_capacity_factor = 1.0
+        print("Warning: moe_expert_capacity_factor is not set, using 1.0 for memory tracing.")
+    args.moe_pad_expert_input_to_capacity = True
+
+    if args.moe_token_dispatcher_type != "alltoall":
+        args.moe_token_dispatcher_type = "alltoall"
+        print("Warning: moe_token_dispatcher_type is set to alltoall for memory tracing.")
+    args.moe_enable_deepep = False
+
+    # Training related
+    args.exit_signal_handler = False
+    args.exit_duration_in_mins = None
+
 def validate_args(args, defaults={}):
 
     # Temporary
@@ -948,6 +1004,9 @@ def validate_args(args, defaults={}):
             # optimizer state in the CPU memory of DP rank 0.
             assert args.use_dist_ckpt
 
+    if args.memory_tracing:
+        update_arguments_for_memory_tracing(args)
+
     # Checkpointing
     if args.ckpt_fully_parallel_save_deprecated and args.rank == 0:
         print('--ckpt-fully-parallel-save flag is deprecated and has no effect.'
@@ -1618,6 +1677,10 @@ def _add_regularization_args(parser):
 def _add_training_args(parser):
     group = parser.add_argument_group(title='training')
 
+    group.add_argument('--memory-tracing', action='store_true',
+                       help='If set, memory tracing mode is enabled and on actual computation is performed.')
+    group.add_argument('--save-peak-memory-snapshot', type=str, default=None,
+                       help='If set, save peak memory snapshot to file.')
     group.add_argument('--micro-batch-size', type=int, default=None,
                        help='Batch size per model instance (local batch size). '
                        'Global batch size is local batch size times data '
diff --git a/megatron/training/initialize.py b/megatron/training/initialize.py
index abca159..9bd55c9 100644
--- a/megatron/training/initialize.py
+++ b/megatron/training/initialize.py
@@ -332,6 +332,12 @@ def _initialize_distributed(get_embedding_ranks, get_position_embedding_ranks, s
             'rank': args.rank,
             'timeout': timedelta(minutes=args.distributed_timeout_minutes),
         }
+        if args.memory_tracing:
+            # Use fake distributed backend for memory tracing
+            from torch.testing._internal.distributed.fake_pg import FakeStore
+            store = FakeStore()
+            init_process_group_kwargs['backend'] = 'fake'
+            init_process_group_kwargs['store'] = store
 
         torch.distributed.init_process_group(**init_process_group_kwargs)
         inprocess_restart.maybe_force_nccl_backend_init(device_id)
diff --git a/megatron/training/training.py b/megatron/training/training.py
index 2a8d995..4785958 100644
--- a/megatron/training/training.py
+++ b/megatron/training/training.py
@@ -11,6 +11,7 @@ import math
 import os
 import sys
 from typing import List, Optional
+import contextlib
 
 import torch.distributed
 from .log_handler import CustomHandler
@@ -123,6 +124,10 @@ from .global_vars import (
     get_wandb_writer,
     get_one_logger,
 )
+try:
+    from memory_profiler import MemoryTracer
+except ImportError:
+    raise ImportError("MemoryTracer not found. Please install memory-profiler.")
 from . import one_logger_utils
 
 from . import ft_integration
@@ -802,9 +807,19 @@ def pretrain(
     # Model, optimizer, and learning rate.
     timers('model-and-optimizer-setup', log_level=0).start(barrier=True)
     app_metrics['app_build_optimizer_start_time'] = one_logger_utils.get_timestamp_in_ms()
-    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
-        model_provider, model_type, checkpointing_context=checkpointing_context
-    )
+    if args.memory_tracing:
+        device = torch.cuda.current_device()
+        estimator = MemoryTracer(device=device)
+    else:
+        estimator = contextlib.nullcontext()
+
+    context_phase = estimator.track_phase("setup_model_and_optimizer") if args.memory_tracing else contextlib.nullcontext()
+    with estimator, context_phase:
+        model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
+            model_provider, model_type, checkpointing_context=checkpointing_context
+        )
+    for model_chunk in model:
+        unwrapped_model = unwrap_model(model_chunk)
 
     timers('model-and-optimizer-setup').stop()
     print_datetime('after model, optimizer, and learning rate ' 'scheduler are built')
@@ -860,18 +875,20 @@ def pretrain(
 
         iteration = 0
         if args.do_train and args.train_iters > 0:
-            iteration, num_floating_point_operations_so_far = train(
-                forward_step_func,
-                model,
-                optimizer,
-                opt_param_scheduler,
-                train_data_iterator,
-                valid_data_iterator,
-                process_non_loss_data_func,
-                config,
-                checkpointing_context,
-                non_loss_data_func,
-            )
+            with estimator:
+                iteration, num_floating_point_operations_so_far = train(
+                    forward_step_func,
+                    model,
+                    optimizer,
+                    opt_param_scheduler,
+                    train_data_iterator,
+                    valid_data_iterator,
+                    process_non_loss_data_func,
+                    config,
+                    checkpointing_context,
+                    non_loss_data_func,
+                    estimator
+                )
 
         print_datetime('after training is done')
 
@@ -1344,7 +1361,7 @@ def dummy_train_step(data_iterator):
         batch = get_batch_on_this_cp_rank(batch)
 
 
-def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_scheduler, config):
+def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_scheduler, config, estimator):
     """Single training step."""
     args = get_args()
     timers = get_timers()
@@ -1379,17 +1396,19 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
 
         # Forward pass.
         forward_backward_func = get_forward_backward_func()
-        losses_reduced = forward_backward_func(
-            forward_step_func=forward_step_func,
-            data_iterator=data_iterator,
-            model=model,
-            num_microbatches=get_num_microbatches(),
-            seq_length=args.seq_length,
-            micro_batch_size=args.micro_batch_size,
-            decoder_seq_length=args.decoder_seq_length,
-            forward_only=False,
-            adjust_tensor_shapes_fn=adjust_tensor_shapes_fn,
-        )
+        phase_context = estimator.track_phase(f"forward_backward_iter_{args.curr_iteration}") if args.memory_tracing else contextlib.nullcontext()
+        with phase_context:
+            losses_reduced = forward_backward_func(
+                forward_step_func=forward_step_func,
+                data_iterator=data_iterator,
+                model=model,
+                num_microbatches=get_num_microbatches(),
+                seq_length=args.seq_length,
+                micro_batch_size=args.micro_batch_size,
+                decoder_seq_length=args.decoder_seq_length,
+                forward_only=False,
+                adjust_tensor_shapes_fn=adjust_tensor_shapes_fn
+            )
     should_checkpoint, should_exit, exit_code = rerun_state_machine.should_checkpoint_and_exit()
     if should_exit:
         return {}, True, should_checkpoint, should_exit, exit_code, None, None
@@ -1406,7 +1425,13 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
     # Update parameters.
 
     timers('optimizer', log_level=1).start(barrier=args.barrier_with_L1_time)
-    update_successful, grad_norm, num_zeros_in_grad = optimizer.step()
+    phase_context = estimator.track_phase(f"optimizer_step_iter_{args.curr_iteration}") if args.memory_tracing else contextlib.nullcontext()
+    with phase_context:
+        update_successful, grad_norm, num_zeros_in_grad = optimizer.step()
+    if args.memory_tracing:
+        estimator.print_memory_stats(f"train iter {args.curr_iteration}")
+        if args.save_peak_memory_snapshot:
+            estimator.memory_dispatch_mode.save_peak_memory_snapshot_to_file(args.save_peak_memory_snapshot, min_memory_mb=1)
     timers('optimizer').stop()
 
     # when freezing sub-models we may have a mixture of successful and unsucessful ranks,
@@ -2001,6 +2026,7 @@ def train(
     config,
     checkpointing_context,
     non_loss_data_func,
+    estimator
 ):
     """Training function: run train_step desired number of times, run validation, checkpoint."""
     args = get_args()
@@ -2227,7 +2253,7 @@ def train(
             grad_norm,
             num_zeros_in_grad,
         ) = train_step(
-            forward_step_func, train_data_iterator, model, optimizer, opt_param_scheduler, config
+            forward_step_func, train_data_iterator, model, optimizer, opt_param_scheduler, config, estimator
         )
         ft_integration.on_training_step_end()
         if should_checkpoint:
diff --git a/pretrain_gpt.py b/pretrain_gpt.py
index 72c539f..365a14a 100644
--- a/pretrain_gpt.py
+++ b/pretrain_gpt.py
@@ -272,6 +272,46 @@ def forward_step(data_iterator, model: GPTModel):
     global stimer
     with stimer(bdata=True):
         tokens, labels, loss_mask, attention_mask, position_ids = get_batch(data_iterator)
+
+        if args.memory_tracing:
+            from torch._subclasses import FakeTensorMode
+            with FakeTensorMode(allow_non_fake_inputs=True):
+                if tokens is not None:
+                    tokens = torch.empty(
+                        tokens.shape,
+                        dtype=tokens.dtype,
+                        device=tokens.device,
+                        requires_grad=tokens.requires_grad
+                    )
+                if labels is not None:
+                    labels = torch.empty(
+                        labels.shape,
+                        dtype=labels.dtype,
+                        device=labels.device,
+                        requires_grad=labels.requires_grad
+                    )
+                if loss_mask is not None:
+                    loss_mask = torch.empty(
+                        loss_mask.shape,
+                        dtype=loss_mask.dtype,
+                        device=loss_mask.device,
+                        requires_grad=loss_mask.requires_grad
+                    )
+                if attention_mask is not None:
+                    attention_mask = torch.empty(
+                        attention_mask.shape,
+                        dtype=attention_mask.dtype,
+                        device=attention_mask.device,
+                        requires_grad=attention_mask.requires_grad
+                    )
+                if position_ids is not None:
+                    position_ids = torch.empty(
+                        position_ids.shape,
+                        dtype=position_ids.dtype,
+                        device=position_ids.device,
+                        requires_grad=position_ids.requires_grad
+                    )
+
     timers('batch-generator').stop()
 
     with stimer:
-- 
1.8.3.1

