From ace0deb03887de46389b90ca1840cdb55ae1471a Mon Sep 17 00:00:00 2001
From: Dennis Liu <denliu@nvidia.com>
Date: Thu, 4 Sep 2025 23:51:19 -0700
Subject: [PATCH] Add single rank memory-tracing support.

---
 megatron/core/optimizer/__init__.py | 12 ++---
 megatron/training/arguments.py      | 55 +++++++++++++++++++
 megatron/training/initialize.py     |  6 +++
 megatron/training/training.py       | 84 ++++++++++++++++++-----------
 4 files changed, 121 insertions(+), 36 deletions(-)

diff --git a/megatron/core/optimizer/__init__.py b/megatron/core/optimizer/__init__.py
index 940e22302..114e70658 100644
--- a/megatron/core/optimizer/__init__.py
+++ b/megatron/core/optimizer/__init__.py
@@ -138,12 +138,12 @@ def _get_param_groups(
     # so we need to align the param groups across ranks, otherwise we may have
     # runtime error when loading the checkpoint or numerical error when resuming training.
     params_key = list(params_map.keys())
-    gathered_params_key = [None for _ in range(torch.distributed.get_world_size())]
-    torch.distributed.all_gather_object(gathered_params_key, params_key)
-    for keys in gathered_params_key:
-        for key in keys:
-            if key not in params_key:
-                params_key.append(key)
+    # gathered_params_key = [None for _ in range(torch.distributed.get_world_size())]
+    # torch.distributed.all_gather_object(gathered_params_key, params_key)
+    # for keys in gathered_params_key:
+    #     for key in keys:
+    #         if key not in params_key:
+    #             params_key.append(key)
 
     param_groups = []
     for key in params_key:
diff --git a/megatron/training/arguments.py b/megatron/training/arguments.py
index e3116517b..17a2f6841 100644
--- a/megatron/training/arguments.py
+++ b/megatron/training/arguments.py
@@ -332,6 +332,54 @@ def moe_freq_type(x):
         return int(x)
 
 
+def update_arguments_for_memory_tracing(args):
+    # Setting only training for 1 step
+    args.eval_iters = 0
+    args.train_iters = 2
+    args.do_train = True
+
+    # Reducing global batch size to a enough value
+    dp_size = args.world_size // (args.context_parallel_size * args.tensor_model_parallel_size * args.pipeline_model_parallel_size)
+    num_micro_batches = args.global_batch_size // (dp_size * args.micro_batch_size) // args.micro_batch_size
+    min_num_micro_batches = args.pipeline_model_parallel_size * 2
+    if min_num_micro_batches < num_micro_batches:
+        args.global_batch_size = args.global_batch_size * min_num_micro_batches // num_micro_batches
+
+    # Checkpointing related
+    args.load = None
+    args.save = None
+    args.rerun_mode = "disabled"
+    args.check_for_nan_in_loss_and_grad = False
+
+    # Data related
+    args.data_path = None
+    args.mock_data = True
+
+    # Distributed related
+    args.enable_gloo_process_groups = False
+
+    # Logging related
+    args.log_params_norm = False
+    args.log_timers_to_tensorboard = False
+    args.tensorboard_dir = None
+    args.wandb_project = None
+
+    # Kernel related
+    args.tp_comm_overlap = False
+    if args.moe_expert_capacity_factor is None:
+        args.moe_expert_capacity_factor = 1.0
+        print("Warning: moe_expert_capacity_factor is not set, using 1.0 for memory tracing.")
+    args.moe_pad_expert_input_to_capacity = True
+
+    # MoE related
+    if args.moe_token_dispatcher_type != "alltoall":
+        args.moe_token_dispatcher_type = "alltoall"
+        print("Warning: moe_token_dispatcher_type is set to alltoall for memory tracing.")
+    args.moe_enable_deepep = False
+
+    # Training related
+    args.exit_signal_handler = False
+    args.exit_duration_in_mins = None
 def validate_args(args, defaults={}):
 
     # Temporary
@@ -1036,6 +1084,9 @@ def validate_args(args, defaults={}):
             # optimizer state in the CPU memory of DP rank 0.
             assert args.use_dist_ckpt
 
+    if args.memory_tracing:
+        update_arguments_for_memory_tracing(args)
+
     # Checkpointing
     if args.ckpt_fully_parallel_save_deprecated and args.rank == 0:
         print('--ckpt-fully-parallel-save flag is deprecated and has no effect.'
@@ -1819,6 +1870,10 @@ def _add_rl_args(parser):
 def _add_training_args(parser):
     group = parser.add_argument_group(title='training')
 
+    group.add_argument('--memory-tracing', action='store_true',
+                       help='If set, memory tracing mode is enabled and on actual computation is performed.')
+    group.add_argument('--save-peak-memory-snapshot', type=str, default=None,
+                       help='If set, save peak memory snapshot to file.')
     group.add_argument('--micro-batch-size', type=int, default=None,
                        help='Batch size per model instance (local batch size). '
                        'Global batch size is local batch size times data '
diff --git a/megatron/training/initialize.py b/megatron/training/initialize.py
index 85b771448..00805e7bc 100644
--- a/megatron/training/initialize.py
+++ b/megatron/training/initialize.py
@@ -332,6 +332,12 @@ def _initialize_distributed(get_embedding_ranks, get_position_embedding_ranks, s
             'rank': args.rank,
             'timeout': timedelta(minutes=args.distributed_timeout_minutes),
         }
+        if args.memory_tracing:
+            # Use fake distributed backend for memory tracing
+            from torch.testing._internal.distributed.fake_pg import FakeStore
+            store = FakeStore()
+            init_process_group_kwargs['backend'] = 'fake'
+            init_process_group_kwargs['store'] = store
 
         torch.distributed.init_process_group(**init_process_group_kwargs)
         inprocess_restart.maybe_force_nccl_backend_init(device_id)
diff --git a/megatron/training/training.py b/megatron/training/training.py
index f50af683b..b27c4d7a7 100644
--- a/megatron/training/training.py
+++ b/megatron/training/training.py
@@ -10,6 +10,7 @@ import math
 import os
 import sys
 from typing import List, Optional
+import contextlib
 
 import torch.distributed
 
@@ -124,6 +125,10 @@ from .global_vars import (
     get_tokenizer,
     get_energy_monitor,
 )
+try:
+    from memory_profiler import MemoryTracer
+except ImportError:
+    raise ImportError("MemoryTracer not found. Please install memory-profiler.")
 from . import one_logger_utils
 
 from . import ft_integration
@@ -656,9 +661,17 @@ def pretrain(
 
     # Model, optimizer, and learning rate.
     timers('model-and-optimizer-setup', log_level=0).start(barrier=True)
-    model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
-        model_provider, model_type, checkpointing_context=checkpointing_context
-    )
+    if args.memory_tracing:
+        device = f"cuda:{torch.cuda.current_device()}"
+        estimator = MemoryTracer(device=device)
+    else:
+        estimator = contextlib.nullcontext()
+
+    context_phase = estimator.track_phase("setup_model_and_optimizer") if args.memory_tracing else contextlib.nullcontext()
+    with estimator, context_phase:
+        model, optimizer, opt_param_scheduler = setup_model_and_optimizer(
+            model_provider, model_type, checkpointing_context=checkpointing_context
+        )
 
     timers('model-and-optimizer-setup').stop()
     print_datetime('after model, optimizer, and learning rate ' 'scheduler are built')
@@ -718,18 +731,20 @@ def pretrain(
 
         iteration = 0
         if args.do_train and args.train_iters > 0:
-            iteration, num_floating_point_operations_so_far = train(
-                forward_step_func,
-                model,
-                optimizer,
-                opt_param_scheduler,
-                train_data_iterator,
-                valid_data_iterator,
-                process_non_loss_data_func,
-                config,
-                checkpointing_context,
-                non_loss_data_func,
-            )
+            with estimator:
+                iteration, num_floating_point_operations_so_far = train(
+                    forward_step_func,
+                    model,
+                    optimizer,
+                    opt_param_scheduler,
+                    train_data_iterator,
+                    valid_data_iterator,
+                    process_non_loss_data_func,
+                    config,
+                    checkpointing_context,
+                    non_loss_data_func,
+                    estimator
+                )
 
         print_datetime('after training is done')
 
@@ -1209,7 +1224,7 @@ def dummy_train_step(data_iterator):
             batch = get_batch_on_this_cp_rank(batch)
 
 
-def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_scheduler, config, forward_backward_func):
+def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_scheduler, config, forward_backward_func, estimator):
     """Single training step."""
     args = get_args()
     timers = get_timers()
@@ -1238,17 +1253,19 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
                     optim_instance._copy_main_params_to_param_buffer()
 
         # Forward pass.
-        losses_reduced = forward_backward_func(
-            forward_step_func=forward_step_func,
-            data_iterator=data_iterator,
-            model=model,
-            num_microbatches=get_num_microbatches(),
-            seq_length=args.seq_length,
-            micro_batch_size=args.micro_batch_size,
-            decoder_seq_length=args.decoder_seq_length,
-            forward_only=False,
-            adjust_tensor_shapes_fn=adjust_tensor_shapes_fn,
-        )
+        phase_context = estimator.track_phase(f"forward_backward_iter_{args.curr_iteration}") if args.memory_tracing else contextlib.nullcontext()
+        with phase_context:
+            losses_reduced = forward_backward_func(
+                forward_step_func=forward_step_func,
+                data_iterator=data_iterator,
+                model=model,
+                num_microbatches=get_num_microbatches(),
+                seq_length=args.seq_length,
+                micro_batch_size=args.micro_batch_size,
+                decoder_seq_length=args.decoder_seq_length,
+                forward_only=False,
+                adjust_tensor_shapes_fn=adjust_tensor_shapes_fn
+            )
     should_checkpoint, should_exit, exit_code = rerun_state_machine.should_checkpoint_and_exit()
     if should_exit:
         return {}, True, should_checkpoint, should_exit, exit_code, None, None
@@ -1265,7 +1282,13 @@ def train_step(forward_step_func, data_iterator, model, optimizer, opt_param_sch
     # Update parameters.
 
     timers('optimizer', log_level=1).start(barrier=args.barrier_with_L1_time)
-    update_successful, grad_norm, num_zeros_in_grad = optimizer.step()
+    phase_context = estimator.track_phase(f"optimizer_step_iter_{args.curr_iteration}") if args.memory_tracing else contextlib.nullcontext()
+    with phase_context:
+        update_successful, grad_norm, num_zeros_in_grad = optimizer.step()
+    if args.memory_tracing:
+        estimator.print_memory_stats(f"train iter {args.curr_iteration}")
+        if args.save_peak_memory_snapshot:
+            estimator.memory_dispatch_mode.save_peak_memory_snapshot_to_file(args.save_peak_memory_snapshot, min_memory_mb=1)
     timers('optimizer').stop()
 
     # when freezing sub-models we may have a mixture of successful and unsucessful ranks,
@@ -1526,7 +1549,7 @@ def training_log(
             mtp_loss_scale, iteration, writer, wandb_writer, total_loss_dict
         )
     if iteration % args.log_interval == 0:
-        if args.record_memory_history and is_last_rank():
+        if args.record_memory_history:
             snapshot = torch.cuda.memory._snapshot()
             from pickle import dump
 
@@ -1905,6 +1928,7 @@ def train(
     config,
     checkpointing_context,
     non_loss_data_func,
+    estimator
 ):
     """Training function: run train_step desired number of times, run validation, checkpoint."""
     args = get_args()
@@ -2195,7 +2219,7 @@ def train(
             grad_norm,
             num_zeros_in_grad,
         ) = train_step(
-            forward_step_func, train_data_iterator, model, optimizer, opt_param_scheduler, config, forward_backward_func
+            forward_step_func, train_data_iterator, model, optimizer, opt_param_scheduler, config, forward_backward_func, estimator
         )
         ft_integration.on_training_step_end()
         if should_checkpoint:
-- 
2.34.1

