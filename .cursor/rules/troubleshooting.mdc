---
description: Common issues and debugging strategies for memory profiling
---
# Troubleshooting Guide

## Common Issues

### FakeTensor Errors
**Problem**: "FakeTensor doesn't support data-dependent operations"
**Solution**: 
- Identify the problematic operation in the stack trace
- Add custom handling in the appropriate plugin
- Use `ProblematicOpsDispatchMode` to identify issues

### Memory Estimation Inaccuracy
**Problem**: Estimated memory differs significantly from actual
**Causes**:
1. Missing operation coverage
2. Incorrect storage size calculation
3. External library allocations not tracked

**Debugging**:
```python
# Enable detailed logging
import logging
from memory_profiler.core.logger import configure_logging
configure_logging(level=logging.DEBUG)

# Compare with actual execution
tracer.compare_with_actual = True
```

### Plugin Loading Issues
**Problem**: Plugin not being applied
**Check**:
1. Library is installed: `import library_name`
2. Plugin is registered: Check tracer.plugins list
3. Patches are applied: Verify in debug logs

### Distributed Training Issues
**Problem**: Incorrect memory estimation for multi-GPU
**Verify**:
- Environment variables: WORLD_SIZE, RANK
- Tensor sharding is correctly simulated
- Communication ops are properly mocked

## Debugging Tools

### Memory Snapshot Analysis
```python
# Save detailed snapshot
tracer.save_memory_snapshot("debug_snapshot.json", include_stack_trace=True)

# Analyze with visualizer
# streamlit run tools/visualizer.py -- --file debug_snapshot.json
```

### Operation Tracking
```python
# Track specific operations
tracer.memory_dispatch_mode.track_ops = ["aten::linear", "aten::matmul"]

# Log all operations
tracer.memory_dispatch_mode.log_all_ops = True
```

### Module Attribution
```python
# Get per-module memory usage
module_memory = tracer.mod_tracker.get_module_memory_summary()
for name, memory in module_memory.items():
    print(f"{name}: {memory / 1024**2:.2f} MB")
```

## Performance Optimization
- Disable stack trace collection for faster execution
- Use batch processing for multiple models
- Cache fake tensor creation for repeated shapes
- Minimize hook overhead with selective registration
