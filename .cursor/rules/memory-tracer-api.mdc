---
description: API usage patterns and best practices for MemoryTracer
---
# MemoryTracer API Usage Guide

## Basic Usage Pattern
```python
from memory_profiler import MemoryTracer

# Create tracer instance
tracer = MemoryTracer(device="cuda", use_fake_tensor=True)

# Use as context manager
with tracer:
    # Your model and training code here
    model = create_model()
    optimizer = create_optimizer(model.parameters())
    
    # Register hooks for module-level tracking
    hooks = tracer.memory_dispatch_mode.register_hooks_to_module(model)
    
    # Run training iteration
    loss = train_step(model, data)
    
    # Clean up hooks
    tracer.memory_dispatch_mode.remove_hooks(hooks)

# Print memory statistics
tracer.print_memory_stats()
```

## Phase Tracking
- Use `tracer.track_phase("phase_name")` to mark different training phases
- Common phases: "forward", "backward", "optimizer_step"
- Phases help identify memory bottlenecks

## Memory Snapshots
- Save snapshots: `tracer.save_memory_snapshot(filepath)`
- Snapshots include tensor details, stack traces, and module attribution
- Use tools/visualizer.py to analyze snapshots

## Plugin System
- Plugins are automatically loaded based on available libraries
- Manual plugin registration: `tracer.register_plugin(MyPlugin())`
- Plugins handle library-specific operations and patches

## Distributed Training
- Set WORLD_SIZE and RANK environment variables
- Single process can simulate multi-GPU training
- Supports TP (Tensor Parallel), PP (Pipeline Parallel), EP (Expert Parallel)

## Best Practices
1. Always use context manager to ensure proper cleanup
2. Register hooks before training starts
3. Track phases for detailed analysis
4. Save snapshots at peak memory points
5. Use fake tensors for estimation without GPU
6. Clean up hooks after training iteration
